{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Speech-to-Text Finetune Blueprint","text":"<p>Blueprints empower developers to easily integrate AI capabilities into their projects using open-source models and tools.</p> <p>These docs are your companion to mastering the Speech-to-Text Finetune Blueprint \u2014a local-friendly approach for finetuning an STT model on your own data or on CommonVoice data.</p>"},{"location":"#built-with","title":"Built with","text":""},{"location":"#get-started-quickly","title":"\ud83d\ude80 Get Started Quickly","text":""},{"location":"#start-transcribing-with-huggingface-models-or-finetune-your-own-custom-stt-models-in-minutes","title":"Start transcribing with HuggingFace models or finetune your own custom STT models in minutes:","text":"<ul> <li>Getting Started: Quick setup and installation instructions.</li> </ul>"},{"location":"#understand-the-system","title":"\ud83d\udd0d Understand the System","text":""},{"location":"#dive-deeper-into-how-the-blueprint-works","title":"Dive deeper into how the Blueprint works:","text":"<ul> <li>Step-by-Step Guide: A detailed breakdown of how to use the Blueprint with a suggested user-flow.</li> <li>API Reference: Explore the technical details of the core modules.</li> </ul>"},{"location":"#make-it-yours","title":"\ud83c\udfa8 Make It Yours","text":""},{"location":"#customize-the-blueprint-to-fit-your-needs","title":"Customize the Blueprint to fit your needs:","text":"<ul> <li>Customization Guide: Coming Soon</li> </ul>"},{"location":"#join-the-community","title":"\ud83c\udf1f Join the Community","text":""},{"location":"#help-shape-the-future-of-blueprints","title":"Help shape the future of Blueprints:","text":"<ul> <li>Future Features &amp; Contributions: Learn about exciting upcoming features and how to contribute to the project.</li> </ul> <p>Have more questions? Reach out to us on Discord and we'll see how we can help:</p> <p></p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#speech_to_text_finetune.config","title":"<code>speech_to_text_finetune.config</code>","text":""},{"location":"api/#speech_to_text_finetune.config.Config","title":"<code>Config</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Store configuration used for finetuning</p> <p>Attributes:</p> Name Type Description <code>model_id</code> <code>str</code> <p>HF model id of a Whisper model used for finetuning</p> <code>dataset_id</code> <code>str</code> <p>HF dataset id of a Common Voice dataset version, ideally from the mozilla-foundation repo</p> <code>dataset_source</code> <code>str</code> <p>can be \"HF\" or \"local\", to determine from where to fetch the dataset</p> <code>language</code> <code>str</code> <p>registered language string that is supported by the Common Voice dataset</p> <code>repo_name</code> <code>str</code> <p>used both for local dir and HF, \"default\" will create a name based on the model and language id</p> <code>training_hp</code> <code>TrainingConfig</code> <p>store selective hyperparameter values from Seq2SeqTrainingArguments</p> Source code in <code>src/speech_to_text_finetune/config.py</code> <pre><code>class Config(BaseModel):\n    \"\"\"\n    Store configuration used for finetuning\n\n    Attributes:\n        model_id: HF model id of a Whisper model used for finetuning\n        dataset_id: HF dataset id of a Common Voice dataset version, ideally from the mozilla-foundation repo\n        dataset_source: can be \"HF\" or \"local\", to determine from where to fetch the dataset\n        language: registered language string that is supported by the Common Voice dataset\n        repo_name: used both for local dir and HF, \"default\" will create a name based on the model and language id\n        training_hp: store selective hyperparameter values from Seq2SeqTrainingArguments\n    \"\"\"\n\n    model_id: str\n    dataset_id: str\n    dataset_source: str\n    language: str\n    repo_name: str\n    training_hp: TrainingConfig\n</code></pre>"},{"location":"api/#speech_to_text_finetune.config.TrainingConfig","title":"<code>TrainingConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>More info at https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments</p> Source code in <code>src/speech_to_text_finetune/config.py</code> <pre><code>class TrainingConfig(BaseModel):\n    \"\"\"\n    More info at https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments\n    \"\"\"\n\n    push_to_hub: bool\n    hub_private_repo: bool\n    max_steps: int\n    per_device_train_batch_size: int\n    gradient_accumulation_steps: int\n    learning_rate: float\n    warmup_steps: int\n    gradient_checkpointing: bool\n    fp16: bool\n    eval_strategy: str\n    per_device_eval_batch_size: int\n    predict_with_generate: bool\n    generation_max_length: int\n    save_steps: int\n    logging_steps: int\n    load_best_model_at_end: bool\n    save_total_limit: int\n    metric_for_best_model: str\n    greater_is_better: bool\n</code></pre>"},{"location":"api/#speech_to_text_finetune.data_process","title":"<code>speech_to_text_finetune.data_process</code>","text":""},{"location":"api/#speech_to_text_finetune.data_process.DataCollatorSpeechSeq2SeqWithPadding","title":"<code>DataCollatorSpeechSeq2SeqWithPadding</code>  <code>dataclass</code>","text":"<p>Data Collator class in the format expected by Seq2SeqTrainer used for processing input data and labels in batches while finetuning. More info here:</p> Source code in <code>src/speech_to_text_finetune/data_process.py</code> <pre><code>@dataclass\nclass DataCollatorSpeechSeq2SeqWithPadding:\n    \"\"\"\n    Data Collator class in the format expected by Seq2SeqTrainer used for processing\n    input data and labels in batches while finetuning. More info here:\n    \"\"\"\n\n    processor: WhisperProcessor\n    decoder_start_token_id: int\n\n    def __call__(\n        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]\n    ) -&gt; Dict[str, torch.Tensor]:\n        # split inputs and labels since they have to be of different lengths and need different padding methods\n        input_features = [\n            {\"input_features\": feature[\"input_features\"]} for feature in features\n        ]\n        batch = self.processor.feature_extractor.pad(\n            input_features, return_tensors=\"pt\"\n        )\n\n        # get the tokenized label sequences\n        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n        # pad the labels to max length\n        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n\n        # replace padding with -100 to ignore loss correctly\n        labels = labels_batch[\"input_ids\"].masked_fill(\n            labels_batch.attention_mask.ne(1), -100\n        )\n\n        # if labels already have a bos token, remove it since its appended later\n        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n            labels = labels[:, 1:]\n\n        batch[\"labels\"] = labels\n\n        return batch\n</code></pre>"},{"location":"api/#speech_to_text_finetune.data_process._process_inputs_and_labels_for_whisper","title":"<code>_process_inputs_and_labels_for_whisper(batch, feature_extractor, tokenizer)</code>","text":"<p>Use Whisper's feature extractor to transform the input audio arrays into log-Mel spectrograms  and the tokenizer to transform the text-label into tokens. This function is expected to be called using  the .map method in order to process the data batch by batch.</p> Source code in <code>src/speech_to_text_finetune/data_process.py</code> <pre><code>def _process_inputs_and_labels_for_whisper(\n    batch: Dict, feature_extractor: WhisperFeatureExtractor, tokenizer: WhisperTokenizer\n) -&gt; Dict:\n    \"\"\"\n    Use Whisper's feature extractor to transform the input audio arrays into log-Mel spectrograms\n     and the tokenizer to transform the text-label into tokens. This function is expected to be called using\n     the .map method in order to process the data batch by batch.\n    \"\"\"\n    audio = batch[\"audio\"]\n\n    batch[\"input_features\"] = feature_extractor(\n        audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]\n    ).input_features[0]\n\n    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n    return batch\n</code></pre>"},{"location":"api/#speech_to_text_finetune.data_process.load_common_voice","title":"<code>load_common_voice(dataset_id, language_id)</code>","text":"<p>Load the default train+validation split used for finetuning and a test split used for evaluation. Args:     dataset_id: official Common Voice dataset id from the mozilla-foundation organisation from Hugging Face     language_id: a registered language identifier from Common Voice (most often in ISO-639 format)</p> <p>Returns:</p> Name Type Description <code>DatasetDict</code> <code>DatasetDict</code> <p>HF Dataset dictionary that consists of two distinct Datasets</p> Source code in <code>src/speech_to_text_finetune/data_process.py</code> <pre><code>def load_common_voice(dataset_id: str, language_id: str) -&gt; DatasetDict:\n    \"\"\"\n    Load the default train+validation split used for finetuning and a test split used for evaluation.\n    Args:\n        dataset_id: official Common Voice dataset id from the mozilla-foundation organisation from Hugging Face\n        language_id: a registered language identifier from Common Voice (most often in ISO-639 format)\n\n    Returns:\n        DatasetDict: HF Dataset dictionary that consists of two distinct Datasets\n    \"\"\"\n    common_voice = DatasetDict()\n\n    common_voice[\"train\"] = load_dataset(\n        dataset_id, language_id, split=\"train+validation\", trust_remote_code=True\n    )\n    common_voice[\"test\"] = load_dataset(\n        dataset_id, language_id, split=\"test\", trust_remote_code=True\n    )\n    common_voice = common_voice.remove_columns(\n        [\n            \"accent\",\n            \"age\",\n            \"client_id\",\n            \"down_votes\",\n            \"gender\",\n            \"locale\",\n            \"path\",\n            \"segment\",\n            \"up_votes\",\n        ]\n    )\n\n    return common_voice\n</code></pre>"},{"location":"api/#speech_to_text_finetune.data_process.load_local_dataset","title":"<code>load_local_dataset(dataset_dir, train_split=0.8)</code>","text":"<p>Load sentences and accompanied recorded audio files into a pandas DataFrame, then split into train/test and finally load it into two distinct train Dataset and test Dataset.</p> <p>Sentences and audio files should be indexed like this: :  should be accompanied by rec_.wav <p>Parameters:</p> Name Type Description Default <code>dataset_dir</code> <code>str</code> <p>path to the local dataset, expecting a text.csv and .wav files under the directory</p> required <code>train_split</code> <code>float</code> <p>percentage split of the dataset to train+validation and test set</p> <code>0.8</code> <p>Returns:</p> Name Type Description <code>DatasetDict</code> <code>DatasetDict</code> <p>HF Dataset dictionary in the same exact format as the Common Voice dataset from load_common_voice</p> Source code in <code>src/speech_to_text_finetune/data_process.py</code> <pre><code>def load_local_dataset(dataset_dir: str, train_split: float = 0.8) -&gt; DatasetDict:\n    \"\"\"\n    Load sentences and accompanied recorded audio files into a pandas DataFrame, then split into train/test and finally\n    load it into two distinct train Dataset and test Dataset.\n\n    Sentences and audio files should be indexed like this: &lt;index&gt;: &lt;sentence&gt; should be accompanied by rec_&lt;index&gt;.wav\n\n    Args:\n        dataset_dir (str): path to the local dataset, expecting a text.csv and .wav files under the directory\n        train_split (float): percentage split of the dataset to train+validation and test set\n\n    Returns:\n        DatasetDict: HF Dataset dictionary in the same exact format as the Common Voice dataset from load_common_voice\n    \"\"\"\n    text_file = dataset_dir + \"/text.csv\"\n\n    dataframe = pd.read_csv(text_file)\n    audio_files = sorted(\n        [f\"{dataset_dir}/{f}\" for f in os.listdir(dataset_dir) if f.endswith(\".wav\")]\n    )\n\n    dataframe[\"audio\"] = audio_files\n    train_index = round(len(dataframe) * train_split)\n\n    my_data = DatasetDict()\n    my_data[\"train\"] = Dataset.from_pandas(dataframe[:train_index])\n    my_data[\"test\"] = Dataset.from_pandas(dataframe[train_index:])\n\n    return my_data\n</code></pre>"},{"location":"api/#speech_to_text_finetune.data_process.process_dataset","title":"<code>process_dataset(dataset, feature_extractor, tokenizer)</code>","text":"<p>Process dataset to the expected format by a Whisper model.</p> Source code in <code>src/speech_to_text_finetune/data_process.py</code> <pre><code>def process_dataset(\n    dataset: DatasetDict,\n    feature_extractor: WhisperFeatureExtractor,\n    tokenizer: WhisperTokenizer,\n) -&gt; DatasetDict:\n    \"\"\"\n    Process dataset to the expected format by a Whisper model.\n    \"\"\"\n    # Create a new column that consists of the resampled audio samples in the right sample rate for whisper\n    dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n\n    dataset = dataset.map(\n        _process_inputs_and_labels_for_whisper,\n        fn_kwargs={\"feature_extractor\": feature_extractor, \"tokenizer\": tokenizer},\n        remove_columns=dataset.column_names[\"train\"],\n        num_proc=1,\n    )\n    return dataset\n</code></pre>"},{"location":"api/#speech_to_text_finetune.finetune_whisper","title":"<code>speech_to_text_finetune.finetune_whisper</code>","text":""},{"location":"api/#speech_to_text_finetune.finetune_whisper.compute_word_error_rate","title":"<code>compute_word_error_rate(pred, tokenizer, metric)</code>","text":"<p>Word Error Rate (wer) is a metric that measures the ratio of errors the ASR model makes given a transcript to the total words spoken. Lower is better. To identify an \"error\" we measure the difference between the ASR generated transcript and the ground truth transcript using the following formula: - S is the number of substitutions (number of words ASR swapped for different words from the ground truth) - D is the number of deletions (number of words ASR skipped / didn't generate compared to the ground truth) - I is the number of insertions (number of additional words ASR generated, not found in the ground truth) - C is the number of correct words (number of words that are identical between ASR and ground truth scripts)</p> <p>then: WER = (S+D+I) / (S+D+C)</p> <p>Note 1: WER can be larger than 1.0, if the number of insertions I is larger than the number of correct words C. Note 2: WER doesn't tell the whole story and is not fully representative of the quality of the ASR model.</p> <p>Parameters:</p> Name Type Description Default <code>pred</code> <code>EvalPrediction</code> <p>Transformers object that holds predicted tokens and ground truth labels</p> required <code>tokenizer</code> <code>WhisperTokenizer</code> <p>Whisper tokenizer used to decode tokens to strings</p> required <code>metric</code> <code>EvaluationModule</code> <p>module that calls the computing function for WER</p> required <p>Returns:     wer (Dict): computed WER metric</p> Source code in <code>src/speech_to_text_finetune/finetune_whisper.py</code> <pre><code>def compute_word_error_rate(\n    pred: EvalPrediction, tokenizer: WhisperTokenizer, metric: EvaluationModule\n) -&gt; Dict:\n    \"\"\"\n    Word Error Rate (wer) is a metric that measures the ratio of errors the ASR model makes given a transcript to the\n    total words spoken. Lower is better.\n    To identify an \"error\" we measure the difference between the ASR generated transcript and the\n    ground truth transcript using the following formula:\n    - S is the number of substitutions (number of words ASR swapped for different words from the ground truth)\n    - D is the number of deletions (number of words ASR skipped / didn't generate compared to the ground truth)\n    - I is the number of insertions (number of additional words ASR generated, not found in the ground truth)\n    - C is the number of correct words (number of words that are identical between ASR and ground truth scripts)\n\n    then: WER = (S+D+I) / (S+D+C)\n\n    Note 1: WER can be larger than 1.0, if the number of insertions I is larger than the number of correct words C.\n    Note 2: WER doesn't tell the whole story and is not fully representative of the quality of the ASR model.\n\n    Args:\n        pred (EvalPrediction): Transformers object that holds predicted tokens and ground truth labels\n        tokenizer (WhisperTokenizer): Whisper tokenizer used to decode tokens to strings\n        metric (EvaluationModule): module that calls the computing function for WER\n    Returns:\n        wer (Dict): computed WER metric\n    \"\"\"\n    pred_ids = pred.predictions\n    label_ids = pred.label_ids\n\n    label_ids[label_ids == -100] = tokenizer.pad_token_id\n\n    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n\n    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n\n    return {\"wer\": wer}\n</code></pre>"},{"location":"api/#speech_to_text_finetune.finetune_whisper.run_finetuning","title":"<code>run_finetuning(config_path='config.yaml')</code>","text":"<p>Complete pipeline for preprocessing the Common Voice dataset and then finetuning a Whisper model on it.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>yaml filepath that follows the format defined in config.py</p> <code>'config.yaml'</code> <p>Returns:</p> Type Description <code>Tuple[Dict, Dict]</code> <p>Tuple[Dict, Dict]: evaluation metrics from the baseline and the finetuned models</p> Source code in <code>src/speech_to_text_finetune/finetune_whisper.py</code> <pre><code>def run_finetuning(\n    config_path: str = \"config.yaml\",\n) -&gt; Tuple[Dict, Dict]:\n    \"\"\"\n    Complete pipeline for preprocessing the Common Voice dataset and then finetuning a Whisper model on it.\n\n    Args:\n        config_path (str): yaml filepath that follows the format defined in config.py\n\n    Returns:\n        Tuple[Dict, Dict]: evaluation metrics from the baseline and the finetuned models\n    \"\"\"\n    cfg = load_config(config_path)\n\n    language_id = LANGUAGES_NAME_TO_ID[cfg.language]\n\n    if cfg.repo_name == \"default\":\n        cfg.repo_name = f\"{cfg.model_id.split('/')[1]}-{language_id}\"\n    local_output_dir = f\"./artifacts/{cfg.repo_name}\"\n\n    logger.info(f\"Finetuning starts soon, results saved locally at {local_output_dir}\")\n    hf_repo_name = \"\"\n    if cfg.training_hp.push_to_hub:\n        hf_username = get_hf_username()\n        hf_repo_name = f\"{hf_username}/{cfg.repo_name}\"\n        logger.info(\n            f\"Results will also be uploaded in HF at {hf_repo_name}. \"\n            f\"Private repo is set to {cfg.training_hp.hub_private_repo}.\"\n        )\n\n    logger.info(f\"Loading the {cfg.language} subset from the {cfg.dataset_id} dataset.\")\n    if cfg.dataset_source == \"HF\":\n        dataset = load_common_voice(cfg.dataset_id, language_id)\n    elif cfg.dataset_source == \"local\":\n        dataset = load_local_dataset(cfg.dataset_id, train_split=0.8)\n    else:\n        raise ValueError(f\"Unknown dataset source {cfg.dataset_source}\")\n\n    device = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n\n    logger.info(\n        f\"Loading {cfg.model_id} on {device} and configuring it for {cfg.language}.\"\n    )\n    feature_extractor = WhisperFeatureExtractor.from_pretrained(cfg.model_id)\n    tokenizer = WhisperTokenizer.from_pretrained(\n        cfg.model_id, language=cfg.language, task=\"transcribe\"\n    )\n    processor = WhisperProcessor.from_pretrained(\n        cfg.model_id, language=cfg.language, task=\"transcribe\"\n    )\n    model = WhisperForConditionalGeneration.from_pretrained(cfg.model_id)\n\n    model.generation_config.language = cfg.language.lower()\n    model.generation_config.task = \"transcribe\"\n    model.generation_config.forced_decoder_ids = None\n\n    logger.info(\"Preparing dataset...\")\n    dataset = process_dataset(dataset, feature_extractor, tokenizer)\n\n    data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n        processor=processor,\n        decoder_start_token_id=model.config.decoder_start_token_id,\n    )\n\n    training_args = Seq2SeqTrainingArguments(\n        output_dir=local_output_dir,\n        hub_model_id=hf_repo_name,\n        report_to=[\"tensorboard\"],\n        **cfg.training_hp.model_dump(),\n    )\n\n    metric = evaluate.load(\"wer\")\n\n    trainer = Seq2SeqTrainer(\n        args=training_args,\n        model=model,\n        train_dataset=dataset[\"train\"],\n        eval_dataset=dataset[\"test\"],\n        data_collator=data_collator,\n        compute_metrics=partial(\n            compute_word_error_rate, tokenizer=tokenizer, metric=metric\n        ),\n        processing_class=processor.feature_extractor,\n    )\n\n    feature_extractor.save_pretrained(training_args.output_dir)\n    tokenizer.save_pretrained(training_args.output_dir)\n    processor.save_pretrained(training_args.output_dir)\n\n    logger.info(\n        f\"Before finetuning, run evaluation on the baseline model {cfg.model_id} to easily compare performance\"\n        f\" before and after finetuning\"\n    )\n    baseline_eval_results = trainer.evaluate()\n    logger.info(f\"Baseline evaluation complete. Results:\\n\\t {baseline_eval_results}\")\n\n    logger.info(\n        f\"Start finetuning job on {dataset['train'].num_rows} audio samples. Monitor training metrics in real time in \"\n        f\"a local tensorboard server by running in a new terminal: tensorboard --logdir {training_args.output_dir}/runs\"\n    )\n    trainer.train()\n    logger.info(\"Finetuning job complete.\")\n\n    logger.info(f\"Start evaluation on {dataset['test'].num_rows} audio samples.\")\n    eval_results = trainer.evaluate()\n    logger.info(f\"Evaluation complete. Results:\\n\\t {eval_results}\")\n\n    if cfg.training_hp.push_to_hub:\n        logger.info(f\"Uploading model and eval results to HuggingFace: {hf_repo_name}\")\n        trainer.push_to_hub()\n        upload_custom_hf_model_card(\n            hf_repo_name=hf_repo_name,\n            model_id=cfg.model_id,\n            dataset_id=cfg.dataset_id,\n            language_id=language_id,\n            language=cfg.language,\n            n_train_samples=dataset[\"train\"].num_rows,\n            n_eval_samples=dataset[\"test\"].num_rows,\n            baseline_eval_results=baseline_eval_results,\n            ft_eval_results=eval_results,\n        )\n\n    return baseline_eval_results, eval_results\n</code></pre>"},{"location":"api/#speech_to_text_finetune.hf_utils","title":"<code>speech_to_text_finetune.hf_utils</code>","text":""},{"location":"api/#speech_to_text_finetune.hf_utils.get_available_languages_in_cv","title":"<code>get_available_languages_in_cv(dataset_id)</code>","text":"<p>Checks if dictionary with the languages already exists as .json and load it. If not: Downloads a languages.py file from a Common Voice dataset repo which stores all languages available. Then, dynamically imports the file as a module and returns the dictionary defined inside. Since the dictionary is in the format {: } , e.g. {'ab': 'Abkhaz'} We swap to use the full language name as key and the ISO id as value instead. Then save the dictionary as json for easier loading next time and remove languages.py as its no longer necessary <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>It needs to be a specific Common Voice dataset id, e.g. mozilla-foundation/common_voice_17_0</p> required <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict</code> <p>A language mapping dictionary in the format {: } , e.g. {'Abkhaz': 'ab'} Source code in <code>src/speech_to_text_finetune/hf_utils.py</code> <pre><code>def get_available_languages_in_cv(dataset_id: str) -&gt; Dict:\n    \"\"\"\n    Checks if dictionary with the languages already exists as .json and load it. If not:\n    Downloads a languages.py file from a Common Voice dataset repo which stores all languages available.\n    Then, dynamically imports the file as a module and returns the dictionary defined inside.\n    Since the dictionary is in the format {&lt;ISO-639-id&gt;: &lt;Full language name&gt;} , e.g. {'ab': 'Abkhaz'}\n    We swap to use the full language name as key and the ISO id as value instead.\n    Then save the dictionary as json for easier loading next time and remove languages.py as its no longer necessary\n\n    Args:\n        dataset_id: It needs to be a specific Common Voice dataset id, e.g. mozilla-foundation/common_voice_17_0\n\n    Returns:\n        Dict: A language mapping dictionary in the format {&lt;Full language name&gt;: &lt;ISO-639-id&gt;} , e.g. {'Abkhaz': 'ab'}\n    \"\"\"\n    lang_map_file_name = f\"./artifacts/languages_{dataset_id.split('/')[1]}.json\"\n\n    if Path(lang_map_file_name).is_file():\n        logger.info(f\"Found {lang_map_file_name} locally, loading the dictionary.\")\n        with open(lang_map_file_name) as json_file:\n            lang_name_to_id = json.load(json_file)\n        return lang_name_to_id\n\n    logger.info(\n        f\"{lang_map_file_name} not found locally. Downloading it from {dataset_id}...\"\n    )\n    filepath = hf_hub_download(\n        repo_id=dataset_id, filename=\"languages.py\", repo_type=\"dataset\", local_dir=\".\"\n    )\n    # Dynamically load LANGUAGES dictionary from languages.py as module\n    spec = importlib.util.spec_from_file_location(\"languages_map_module\", filepath)\n    module = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(module)\n    lang_id_to_name = module.LANGUAGES\n\n    # Swap keys &lt;&gt; values\n    lang_name_to_id = dict((v, k) for k, v in lang_id_to_name.items())\n\n    logger.info(f\"Saving {lang_map_file_name} locally to use it next time.\")\n    Path(\"./artifacts\").mkdir(exist_ok=True)\n    with open(lang_map_file_name, \"w\") as lang_file:\n        json.dump(lang_name_to_id, lang_file, indent=4)\n\n    # Cleanup\n    os.remove(filepath)\n\n    return lang_name_to_id\n</code></pre>"},{"location":"api/#speech_to_text_finetune.hf_utils.upload_custom_hf_model_card","title":"<code>upload_custom_hf_model_card(hf_repo_name, model_id, dataset_id, language_id, language, n_train_samples, n_eval_samples, baseline_eval_results, ft_eval_results)</code>","text":"<p>Create and upload a custom Model Card (https://huggingface.co/docs/hub/model-cards) to the Hugging Face repo of the finetuned model that highlights the evaluation results before and after finetuning.</p> Source code in <code>src/speech_to_text_finetune/hf_utils.py</code> <pre><code>def upload_custom_hf_model_card(\n    hf_repo_name: str,\n    model_id: str,\n    dataset_id: str,\n    language_id: str,\n    language: str,\n    n_train_samples: int,\n    n_eval_samples: int,\n    baseline_eval_results: Dict,\n    ft_eval_results: Dict,\n) -&gt; None:\n    \"\"\"\n    Create and upload a custom Model Card (https://huggingface.co/docs/hub/model-cards) to the Hugging Face repo\n    of the finetuned model that highlights the evaluation results before and after finetuning.\n    \"\"\"\n    card_metadata = ModelCardData(\n        model_name=f\"Finetuned {model_id} on {language}\",\n        base_model=model_id,\n        datasets=[dataset_id],\n        language=language_id,\n        license=\"apache-2.0\",\n        library_name=\"transformers\",\n        eval_results=[\n            EvalResult(\n                task_type=\"automatic-speech-recognition\",\n                task_name=\"Speech-to-Text\",\n                dataset_type=\"common_voice\",\n                dataset_name=f\"Common Voice ({language})\",\n                metric_type=\"wer\",\n                metric_value=round(ft_eval_results[\"eval_wer\"], 3),\n            )\n        ],\n    )\n    content = f\"\"\"\n---\n{card_metadata.to_yaml()}\n---\n\n# Finetuned {model_id} on {n_train_samples} {language} training audio samples from {dataset_id}.\n\nThis model was created from the Mozilla.ai Blueprint:\n[speech-to-text-finetune](https://github.com/mozilla-ai/speech-to-text-finetune).\n\n## Evaluation results on {n_eval_samples} audio samples of {language}:\n\n### Baseline model (before finetuning) on {language}\n- Word Error Rate: {round(baseline_eval_results[\"eval_wer\"], 3)}\n- Loss: {round(baseline_eval_results[\"eval_loss\"], 3)}\n\n### Finetuned model (after finetuning) on {language}\n- Word Error Rate: {round(ft_eval_results[\"eval_wer\"], 3)}\n- Loss: {round(ft_eval_results[\"eval_loss\"], 3)}\n\"\"\"\n\n    card = ModelCard(content)\n    card.push_to_hub(hf_repo_name)\n</code></pre>"},{"location":"api/#speech_to_text_finetune.make_local_dataset_app","title":"<code>speech_to_text_finetune.make_local_dataset_app</code>","text":""},{"location":"api/#speech_to_text_finetune.make_local_dataset_app.save_text_audio_to_file","title":"<code>save_text_audio_to_file(audio_input, sentence, dataset_dir)</code>","text":"<p>Save the audio recording in a .wav file using the index of the text sentence in the filename. And save the associated text sentence in a .csv file using the same index.</p> <p>Parameters:</p> Name Type Description Default <code>audio_input</code> <code>Audio</code> <p>Gradio audio object to be converted to audio data and then saved to a .wav file</p> required <code>sentence</code> <code>str</code> <p>The text sentence that will be associated with the audio</p> required <code>dataset_dir</code> <code>str</code> <p>The dataset directory path to store the indexed sentences and the associated audio files</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Status text for Gradio app</p> <code>None</code> <code>None</code> <p>Returning None here will reset the audio module to record again from scratch</p> Source code in <code>src/speech_to_text_finetune/make_local_dataset_app.py</code> <pre><code>def save_text_audio_to_file(\n    audio_input: gr.Audio,\n    sentence: str,\n    dataset_dir: str,\n) -&gt; Tuple[str, None]:\n    \"\"\"\n    Save the audio recording in a .wav file using the index of the text sentence in the filename.\n    And save the associated text sentence in a .csv file using the same index.\n\n    Args:\n        audio_input (gr.Audio): Gradio audio object to be converted to audio data and then saved to a .wav file\n        sentence (str): The text sentence that will be associated with the audio\n        dataset_dir (str): The dataset directory path to store the indexed sentences and the associated audio files\n\n    Returns:\n        str: Status text for Gradio app\n        None: Returning None here will reset the audio module to record again from scratch\n    \"\"\"\n    Path(dataset_dir).mkdir(parents=True, exist_ok=True)\n    text_data_path = Path(f\"{dataset_dir}/text.csv\")\n\n    if text_data_path.is_file():\n        text_df = pd.read_csv(text_data_path)\n    else:\n        text_df = pd.DataFrame(columns=[\"index\", \"sentence\"])\n\n    index = len(text_df)\n    text_df = pd.concat(\n        [text_df, pd.DataFrame([{\"index\": index, \"sentence\": sentence}])],\n        ignore_index=True,\n    )\n    text_df = text_df.sort_values(by=\"index\")\n    text_df.to_csv(f\"{dataset_dir}/text.csv\", index=False)\n\n    audio_filepath = f\"{dataset_dir}/rec_{index}.wav\"\n\n    sr, data = audio_input\n    sf.write(file=audio_filepath, data=data, samplerate=sr)\n\n    return (\n        f\"\"\"\u2705 Updated {dataset_dir}/text.csv \\n\u2705 Saved recording to {audio_filepath}\"\"\",\n        None,\n    )\n</code></pre>"},{"location":"customization/","title":"\ud83c\udfa8 Customization Guide","text":"<p>This Blueprint is designed to be flexible and easily adaptable to your specific needs. This guide will walk you through some key areas you can customize to make the Blueprint your own.</p>"},{"location":"customization/#customization-guide-coming-soon","title":"Customization Guide Coming Soon","text":""},{"location":"customization/#contributing-to-the-blueprint","title":"\ud83e\udd1d Contributing to the Blueprint","text":"<p>Want to help improve or extend this Blueprint? Check out the Future Features &amp; Contributions Guide to see how you can contribute your ideas, code, or feedback to make this Blueprint even better!</p>"},{"location":"future-features-contributions/","title":"\ud83d\ude80 Future Features &amp; Contributions","text":"<p>This Blueprint is an evolving project designed to grow with the help of the open-source community. Whether you\u2019re an experienced developer or just starting, there are many ways you can contribute and help shape the future of this tool.</p>"},{"location":"future-features-contributions/#how-you-can-contribute","title":"\ud83c\udf1f How You Can Contribute","text":""},{"location":"future-features-contributions/#enhance-the-blueprint","title":"\ud83d\udee0\ufe0f Enhance the Blueprint","text":"<ul> <li>Check the Issues page to see if there are feature requests you'd like to implement</li> <li>Refer to our Contribution Guide for more details on contributions</li> </ul>"},{"location":"future-features-contributions/#extensibility-ideas","title":"\ud83c\udfa8 Extensibility Ideas","text":"<p>This Blueprint is designed to be a foundation you can build upon. By extending its capabilities, you can open the door to new applications, improve user experience, and adapt the Blueprint to address other use cases. Here are a few ideas for how you can expand its potential: - Add support for more types of models to finetune, such as w2v2-bert. More info here - Improve the training efficiency and speed using PEFT + LORA. More info here</p> <p>We\u2019d love to see how you can enhance this Blueprint! If you create improvements or extend its capabilities, consider contributing them back to the project so others in the community can benefit from your work. Check out our Contributions Guide to get started!</p>"},{"location":"future-features-contributions/#share-your-ideas","title":"\ud83d\udca1 Share Your Ideas","text":"<p>Got an idea for how this Blueprint could be improved? You can share your suggestions through GitHub Discussions.</p>"},{"location":"future-features-contributions/#build-new-blueprints","title":"\ud83c\udf0d Build New Blueprints","text":"<p>This project is part of a larger initiative to create a collection of reusable starter code solutions that use open-source AI tools. If you\u2019re inspired to create your own Blueprint, you can use the Blueprint-template to get started.</p> <p>Your contributions help make this Blueprint better for everyone \ud83c\udf89</p>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#get-started-with-speech-to-text-finetune-blueprint-using-one-of-the-options-below","title":"Get started with Speech-to-text-finetune Blueprint using one of the options below:","text":""},{"location":"getting-started/#setup-options","title":"Setup options","text":"\u2601\ufe0f Google Colab (GPU)\u2601\ufe0f GitHub Codespaces\ud83d\udcbb Local Installation <p>Finetune a STT model using CommonVoice data by launching the Google Colab notebook below</p> <p>Click the button below to launch the project directly in Google Colab:</p> <p><p></p></p> <p>Click the button below to launch the project directly in GitHub Codespaces:</p> <p><p></p></p> <p>Once the Codespaces environment launches, inside the terminal, install dependencies:</p> <pre><code>pip install -e .\n</code></pre> <p>To load the app for making your own dataset for STT finetuning:</p> <pre><code>python src/speech_to_text_finetune/make_local_dataset_app.py\n</code></pre> <p>To load the Transcription app:</p> <pre><code>python demo/transcribe_app.py\n</code></pre> <p>To run the Finetuning job:</p> <pre><code>python src/speech_to_text_finetune/finetune_whisper.py\n</code></pre> <p>To install the project locally:</p> <pre><code>git clone https://github.com/mozilla-ai/speech-to-text-finetune.git\ncd speech-to-text-finetune\n</code></pre> <p>install dependencies:</p> <pre><code>pip install -e .\n</code></pre> <p>To load the app for making your own dataset for STT finetuning:</p> <pre><code>python src/speech_to_text_finetune/make_local_dataset_app.py\n</code></pre> <p>To load the Transcription app:</p> <pre><code>python demo/transcribe_app.py\n</code></pre> <p>To run the Finetuning job:</p> <pre><code>python src/speech_to_text_finetune/finetune_whisper.py\n</code></pre>"},{"location":"getting-started/#troubleshooting","title":"Troubleshooting","text":"<p>Troubleshooting - TBA</p>"},{"location":"step-by-step-guide/","title":"Step-by-Step Guide: How the Speech-to-Text-Finetune Blueprint Works","text":"<p>This Blueprint enables you to fine-tune a Speech-to-Text (STT) model, using either your own data or the Common Voice dataset. This Step-by-Step guide you through the end-to-end process of finetuning an STT model based on your needs.</p>"},{"location":"step-by-step-guide/#overview","title":"Overview","text":"<p>This blueprint consists of three independent, yet complementary, components:</p> <ol> <li> <p>Transcription app \ud83c\udf99\ufe0f\ud83d\udcdd: A simple UI that lets you record your voice, pick any HF STT/ASR model, and get an instant transcription.</p> </li> <li> <p>Dataset maker app \ud83d\udcc2\ud83c\udfa4: Another UI app that enables you to easily and quickly create your own Speech-to-Text dataset.</p> </li> <li> <p>Finetuning script \ud83d\udee0\ufe0f\ud83e\udd16: A script to finetune your own STT model, either using Common Voice data or your own local data created by the Dataset maker app.</p> </li> </ol>"},{"location":"step-by-step-guide/#step-by-step-guide","title":"Step-by-Step Guide","text":"<p>Visit the Getting Started page for the initial project setup.</p> <p>The following guide is a suggested user-flow for getting the most out of this Blueprint</p>"},{"location":"step-by-step-guide/#step-1-initial-transcription-testing","title":"Step 1 - Initial transcription testing","text":"<p>Start by initially testing the quality of the Speech-to-Text models available in HuggingFace:</p> <ol> <li> <p>Simply execute:</p> <pre><code>python demo/transcribe_app.py\n</code></pre> </li> <li> <p>Select or add the HF model id of your choice</p> </li> <li>Record a sample of your voice and get the transcribed text back. You may find that there are sometimes inaccuracies for your voice/accent/chosen language, indicating the model could benefit form finetuning on additional data.</li> </ol>"},{"location":"step-by-step-guide/#step-2-make-your-local-dataset-for-stt-finetuning","title":"Step 2 - Make your Local Dataset for STT finetuning","text":"<ol> <li> <p>Create your own, local dataset by running this command and following the instructions:</p> <pre><code>python src/speech_to_text_finetune/make_local_dataset_app.py\n</code></pre> </li> <li> <p>Follow the instruction in the app to create at least 10 audio samples, which will be saved locally.</p> </li> </ol>"},{"location":"step-by-step-guide/#step-3-creating-a-finetuned-stt-model-using-your-local-data","title":"Step 3 - Creating a finetuned STT model using your local data","text":"<ol> <li> <p>Configure <code>config.yaml</code> with the model, local data directory and hyperparameters of your choice. Note that if you select <code>push_to_hub: True</code> you need to have an HF account and log in locally. For example:</p> <pre><code>model_id: openai/whisper-tiny\ndataset_id: example_data/custom\ndataset_source: local\nlanguage: English\nrepo_name: default\n\ntraining_hp:\n    push_to_hub: False\n    hub_private_repo: True\n    ...\n</code></pre> </li> <li> <p>Finetune a model by running: <pre><code>python src/speech_to_text_finetune/finetune_whisper.py\n</code></pre></p> </li> </ol>"},{"location":"step-by-step-guide/#step-4-optional-creating-a-finetuned-stt-model-using-commonvoice-data","title":"Step 4 - (Optional) Creating a finetuned STT model using CommonVoice data","text":"<p>Note: A Hugging Face account is required!</p> <ol> <li>Go to the Common Voice dataset repo and ask for explicit access request (should be approved instantly).</li> <li>On Hugging Face create an Access Token</li> <li>In your terminal, run the command <code>huggingface-cli login</code> and follow the instructions to log in to your account.</li> <li>Configure <code>config.yaml</code> with the model, Common Voice dataset repo id of HF and hyperparameters of your choice. For example: <pre><code>model_id = \"openai/whisper-tiny\"\ndataset_id = \"mozilla-foundation/common_voice_17_0\"\nlanguage = \"Greek\"\nrepo_name: default\n\ntraining_hp:\n    push_to_hub: False\n    hub_private_repo: True\n    ...\n</code></pre></li> <li>Finetune a model by running: <pre><code>python src/speech_to_text_finetune/finetune_whisper.py\n</code></pre></li> </ol>"},{"location":"step-by-step-guide/#step-5-evaluate-transcription-accuracy-with-your-finetuned-stt-model","title":"Step 5 - Evaluate transcription accuracy with your finetuned STT model","text":"<ol> <li>Start the Transcription app:  <code>bash python demo/transcribe_app.py</code></li> <li>Provided that <code>push_to_hub: True</code> when you Finetuned, you can select your HuggingFace model-id. If not, you can specify the local path to your model</li> <li>Record a sample of your voice and get the transcribed text back.</li> <li>You can easily switch between models with the same recorded sample to evaluate if the finetuned model has improved transcription accuracy.</li> </ol>"},{"location":"step-by-step-guide/#customizing-the-blueprint","title":"\ud83c\udfa8 Customizing the Blueprint","text":"<p>To better understand how you can tailor this Blueprint to suit your specific needs, please visit the Customization Guide.</p>"},{"location":"step-by-step-guide/#contributing-to-the-blueprint","title":"\ud83e\udd1d Contributing to the Blueprint","text":"<p>Want to help improve or extend this Blueprint? Check out the Future Features &amp; Contributions Guide to see how you can contribute your ideas, code, or feedback to make this Blueprint even better!</p>"},{"location":"step-by-step-guide/#resources-references","title":"\ud83d\udcd6 Resources &amp; References","text":"<p>If you are interested in learning more about this topic, you might find the following resources helpful: - Fine-Tune Whisper For Multilingual ASR with \ud83e\udd17 Transformers (Blog post by HuggingFace which inspired the implementation of the Blueprint!)</p> <ul> <li> <p>Automatic Speech Recognition Course from HuggingFace (Series of Blog posts)</p> </li> <li> <p>Fine-Tuning ASR Models: Key Definitions, Mechanics, and Use Cases (Blog post by Gladia)</p> </li> <li> <p>Active Learning Approach for Fine-Tuning Pre-Trained ASR Model for a low-resourced Language (Paper)</p> </li> <li> <p>Exploration of Whisper fine-tuning strategies for low-resource ASR (Paper)</p> </li> <li> <p>Finetuning Pretrained Model with Embedding of Domain and Language Information for ASR of Very Low-Resource Settings (Paper)</p> </li> </ul>"}]}