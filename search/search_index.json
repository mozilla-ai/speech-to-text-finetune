{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Speech-to-Text Finetune Blueprint","text":"<p>Blueprints empower developers to easily integrate AI capabilities into their projects using open-source models and tools.</p> <p>These docs are your companion to mastering the Speech-to-Text Finetune Blueprint \u2014a local-friendly approach for finetuning an STT model on your own data or on CommonVoice data.</p>"},{"location":"#built-with","title":"Built with","text":""},{"location":"#get-started-quickly","title":"\ud83d\ude80 Get Started Quickly","text":""},{"location":"#start-transcribing-with-huggingface-models-or-finetune-your-own-custom-stt-models-in-minutes","title":"Start transcribing with HuggingFace models or finetune your own custom STT models in minutes:","text":"<ul> <li>Getting Started: Quick setup and installation instructions.</li> </ul>"},{"location":"#understand-the-system","title":"\ud83d\udd0d Understand the System","text":""},{"location":"#dive-deeper-into-how-the-blueprint-works","title":"Dive deeper into how the Blueprint works:","text":"<ul> <li>Step-by-Step Guide: A detailed breakdown of how to use the Blueprint with a suggested user-flow.</li> <li>API Reference: Explore the technical details of the core modules.</li> </ul>"},{"location":"#make-it-yours","title":"\ud83c\udfa8 Make It Yours","text":""},{"location":"#customize-the-blueprint-to-fit-your-needs","title":"Customize the Blueprint to fit your needs:","text":"<ul> <li>Customization Guide: Bring Your Own Dataset!</li> </ul>"},{"location":"#join-the-community","title":"\ud83c\udf1f Join the Community","text":""},{"location":"#help-shape-the-future-of-blueprints","title":"Help shape the future of Blueprints:","text":"<ul> <li>Future Features &amp; Contributions: Learn about exciting upcoming features and how to contribute to the project.</li> </ul> <p>Have more questions? Reach out to us on Discord and we'll see how we can help:</p> <p></p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#speech_to_text_finetune.config","title":"<code>speech_to_text_finetune.config</code>","text":""},{"location":"api/#speech_to_text_finetune.config.Config","title":"<code>Config</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Store configuration used for finetuning</p> <p>Attributes:</p> Name Type Description <code>model_id</code> <code>str</code> <p>HF model id of a Whisper model used for finetuning</p> <code>dataset_id</code> <code>str</code> <p>HF dataset id of a Common Voice dataset version, ideally from the mozilla-foundation repo</p> <code>language</code> <code>str</code> <p>registered language string that is supported by the Common Voice dataset</p> <code>repo_name</code> <code>str</code> <p>used both for local dir and HF, \"default\" will create a name based on the model and language id</p> <code>n_train_samples</code> <code>int</code> <p>explicitly set how many samples to train+validate on. If -1, use all train+val data available</p> <code>n_test_samples</code> <code>int</code> <p>explicitly set how many samples to evaluate on. If -1, use all eval data available</p> <code>training_hp</code> <code>TrainingConfig</code> <p>store selective hyperparameter values from Seq2SeqTrainingArguments</p> Source code in <code>src/speech_to_text_finetune/config.py</code> <pre><code>class Config(BaseModel):\n    \"\"\"\n    Store configuration used for finetuning\n\n    Attributes:\n        model_id: HF model id of a Whisper model used for finetuning\n        dataset_id: HF dataset id of a Common Voice dataset version, ideally from the mozilla-foundation repo\n        language: registered language string that is supported by the Common Voice dataset\n        repo_name: used both for local dir and HF, \"default\" will create a name based on the model and language id\n        n_train_samples: explicitly set how many samples to train+validate on. If -1, use all train+val data available\n        n_test_samples: explicitly set how many samples to evaluate on. If -1, use all eval data available\n        training_hp: store selective hyperparameter values from Seq2SeqTrainingArguments\n    \"\"\"\n\n    model_id: str\n    dataset_id: str\n    language: str\n    repo_name: str\n    n_train_samples: int\n    n_test_samples: int\n    training_hp: TrainingConfig\n</code></pre>"},{"location":"api/#speech_to_text_finetune.config.TrainingConfig","title":"<code>TrainingConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>More info at https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments</p> Source code in <code>src/speech_to_text_finetune/config.py</code> <pre><code>class TrainingConfig(BaseModel):\n    \"\"\"\n    More info at https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments\n    \"\"\"\n\n    push_to_hub: bool\n    hub_private_repo: bool\n    max_steps: int\n    per_device_train_batch_size: int\n    gradient_accumulation_steps: int\n    learning_rate: float\n    warmup_steps: int\n    gradient_checkpointing: bool\n    fp16: bool\n    eval_strategy: str\n    per_device_eval_batch_size: int\n    predict_with_generate: bool\n    generation_max_length: int\n    save_steps: int\n    logging_steps: int\n    load_best_model_at_end: bool\n    save_total_limit: int\n    metric_for_best_model: str\n    greater_is_better: bool\n</code></pre>"},{"location":"api/#speech_to_text_finetune.data_process","title":"<code>speech_to_text_finetune.data_process</code>","text":""},{"location":"api/#speech_to_text_finetune.data_process.DataCollatorSpeechSeq2SeqWithPadding","title":"<code>DataCollatorSpeechSeq2SeqWithPadding</code>  <code>dataclass</code>","text":"<p>Data Collator class in the format expected by Seq2SeqTrainer used for processing input data and labels in batches while finetuning. More info here:</p> Source code in <code>src/speech_to_text_finetune/data_process.py</code> <pre><code>@dataclass\nclass DataCollatorSpeechSeq2SeqWithPadding:\n    \"\"\"\n    Data Collator class in the format expected by Seq2SeqTrainer used for processing\n    input data and labels in batches while finetuning. More info here:\n    \"\"\"\n\n    processor: WhisperProcessor\n\n    def __call__(\n        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]\n    ) -&gt; Dict[str, torch.Tensor]:\n        # split inputs and labels since they have to be of different lengths and need different padding methods\n        # first treat the audio inputs by simply returning torch tensors\n        input_features = [\n            {\"input_features\": feature[\"input_features\"]} for feature in features\n        ]\n        batch = self.processor.feature_extractor.pad(\n            input_features, return_tensors=\"pt\"\n        )\n\n        # get the tokenized label sequences\n        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n        # pad the labels to max length\n        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n\n        # replace padding with -100 to ignore loss correctly\n        labels = labels_batch[\"input_ids\"].masked_fill(\n            labels_batch.attention_mask.ne(1), -100\n        )\n\n        # if bos token is appended in previous tokenization step,\n        # cut bos token here as it's append later anyway\n        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n            labels = labels[:, 1:]\n\n        batch[\"labels\"] = labels\n\n        return batch\n</code></pre>"},{"location":"api/#speech_to_text_finetune.data_process.load_and_proc_hf_fleurs","title":"<code>load_and_proc_hf_fleurs(language_id, n_test_samples, processor, eval_batch_size)</code>","text":"<p>Load only the test split of fleurs on a specific language and process it for Whisper. Args:     language_id (str): a registered language identifier from Fleurs         (see https://huggingface.co/datasets/google/fleurs/blob/main/fleurs.py)     n_test_samples (int): number of samples to use from the test split     processor (WhisperProcessor): Processor from Whisper to process the dataset     eval_batch_size (int): batch size to use for processing the dataset</p> <p>Returns:</p> Name Type Description <code>DatasetDict</code> <code>Dataset</code> <p>HF Dataset</p> Source code in <code>src/speech_to_text_finetune/data_process.py</code> <pre><code>def load_and_proc_hf_fleurs(\n    language_id: str,\n    n_test_samples: int,\n    processor: WhisperProcessor,\n    eval_batch_size: int,\n) -&gt; Dataset:\n    \"\"\"\n    Load only the test split of fleurs on a specific language and process it for Whisper.\n    Args:\n        language_id (str): a registered language identifier from Fleurs\n            (see https://huggingface.co/datasets/google/fleurs/blob/main/fleurs.py)\n        n_test_samples (int): number of samples to use from the test split\n        processor (WhisperProcessor): Processor from Whisper to process the dataset\n        eval_batch_size (int): batch size to use for processing the dataset\n\n    Returns:\n        DatasetDict: HF Dataset\n    \"\"\"\n    fleurs_dataset_id = \"google/fleurs\"\n    if proc_dataset := try_find_processed_version(fleurs_dataset_id, language_id):\n        return proc_dataset\n\n    dataset = load_dataset(\n        fleurs_dataset_id, language_id, trust_remote_code=True, split=\"test\"\n    )\n    dataset = load_subset_of_dataset(dataset, n_test_samples)\n\n    dataset = dataset.rename_column(\n        original_column_name=\"raw_transcription\", new_column_name=\"sentence\"\n    )\n    dataset = dataset.select_columns([\"audio\", \"sentence\"])\n\n    save_proc_dataset_path = _get_hf_proc_dataset_path(fleurs_dataset_id, language_id)\n    logger.info(\"Processing dataset...\")\n    dataset = process_dataset(\n        dataset=dataset,\n        processor=processor,\n        batch_size=eval_batch_size,\n        proc_dataset_path=save_proc_dataset_path,\n    )\n    logger.info(\n        f\"Processed dataset saved at {save_proc_dataset_path}. Future runs of {fleurs_dataset_id} will \"\n        f\"automatically use this processed version.\"\n    )\n    return dataset\n</code></pre>"},{"location":"api/#speech_to_text_finetune.data_process.load_dataset_from_dataset_id","title":"<code>load_dataset_from_dataset_id(dataset_id, language_id=None)</code>","text":"<p>This function loads a dataset, based on the dataset_id and the content of its directory (if it is a local path). Possible cases: 1. The dataset_id is a path to a local, Common Voice dataset directory.</p> <ol> <li> <p>The dataset_id is a path to a local, custom dataset directory.</p> </li> <li> <p>The dataset_id is a HuggingFace dataset ID.</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Path to a processed dataset directory or local dataset directory or HuggingFace dataset ID.</p> required <code>language_id</code> <code>Only used for the HF dataset case</code> <p>Language identifier for the dataset (e.g., 'en' for English)</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DatasetDict</code> <code>DatasetDict</code> <p>A processed dataset ready for training with train/test splits</p> <code>str</code> <code>str</code> <p>Path to save the processed directory</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the dataset cannot be found locally or on HuggingFace</p> Source code in <code>src/speech_to_text_finetune/data_process.py</code> <pre><code>def load_dataset_from_dataset_id(\n    dataset_id: str,\n    language_id: str | None = None,\n) -&gt; Tuple[DatasetDict, str]:\n    \"\"\"\n    This function loads a dataset, based on the dataset_id and the content of its directory (if it is a local path).\n    Possible cases:\n    1. The dataset_id is a path to a local, Common Voice dataset directory.\n\n    2. The dataset_id is a path to a local, custom dataset directory.\n\n    3. The dataset_id is a HuggingFace dataset ID.\n\n    Args:\n        dataset_id: Path to a processed dataset directory or local dataset directory or HuggingFace dataset ID.\n        language_id (Only used for the HF dataset case): Language identifier for the dataset (e.g., 'en' for English)\n\n    Returns:\n        DatasetDict: A processed dataset ready for training with train/test splits\n        str: Path to save the processed directory\n\n    Raises:\n        ValueError: If the dataset cannot be found locally or on HuggingFace\n    \"\"\"\n    try:\n        dataset = _load_local_common_voice(dataset_id)\n        return dataset, _get_local_proc_dataset_path(dataset_id)\n    except FileNotFoundError:\n        pass\n\n    try:\n        dataset = _load_custom_dataset(dataset_id)\n        return dataset, _get_local_proc_dataset_path(dataset_id)\n    except FileNotFoundError:\n        pass\n\n    try:\n        dataset = _load_hf_common_voice(dataset_id, language_id)\n        return dataset, _get_hf_proc_dataset_path(dataset_id, language_id)\n    except HFValidationError:\n        pass\n    except FileNotFoundError:\n        pass\n\n    raise ValueError(\n        f\"Could not find dataset {dataset_id}, neither locally nor at HuggingFace. \"\n        f\"If its a private repo, make sure you are logged in locally.\"\n    )\n</code></pre>"},{"location":"api/#speech_to_text_finetune.data_process.process_dataset","title":"<code>process_dataset(dataset, processor, batch_size, proc_dataset_path)</code>","text":"<p>Process dataset to the expected format by a Whisper model and then save it locally for future use.</p> Source code in <code>src/speech_to_text_finetune/data_process.py</code> <pre><code>def process_dataset(\n    dataset: DatasetDict | Dataset,\n    processor: WhisperProcessor,\n    batch_size: int,\n    proc_dataset_path: str,\n) -&gt; DatasetDict | Dataset:\n    \"\"\"\n    Process dataset to the expected format by a Whisper model and then save it locally for future use.\n    \"\"\"\n    # Create a new column that consists of the resampled audio samples in the right sample rate for whisper\n    dataset = dataset.cast_column(\n        \"audio\", Audio(sampling_rate=processor.feature_extractor.sampling_rate)\n    )\n\n    dataset = dataset.map(\n        _process_inputs_and_labels_for_whisper,\n        fn_kwargs={\"processor\": processor},\n        remove_columns=dataset.column_names[\"train\"]\n        if \"train\" in dataset.column_names\n        else None,\n        batched=True,\n        batch_size=batch_size,\n        num_proc=1,\n    )\n\n    dataset = dataset.filter(\n        _is_audio_in_length_range,\n        input_columns=[\"input_length\"],\n        fn_kwargs={\"max_input_length\": 30.0},\n        num_proc=1,\n    )\n    dataset = dataset.filter(\n        _are_labels_in_length_range,\n        input_columns=[\"labels\"],\n        fn_kwargs={\"max_label_length\": 448},\n        num_proc=1,\n    )\n\n    proc_dataset_path = Path(proc_dataset_path)\n    Path.mkdir(proc_dataset_path, parents=True, exist_ok=True)\n    dataset.save_to_disk(proc_dataset_path)\n    return dataset\n</code></pre>"},{"location":"api/#speech_to_text_finetune.data_process.try_find_processed_version","title":"<code>try_find_processed_version(dataset_id, language_id=None)</code>","text":"<p>Try to load a processed version of the dataset if it exists locally. Check if: 1. The dataset_id is a local path to an already processed dataset directory. or 2. The dataset_id is a path to a local dataset, but a processed version already exists locally. or 3. The dataset_id is a HuggingFace dataset ID, but a processed version already exists locally.</p> Source code in <code>src/speech_to_text_finetune/data_process.py</code> <pre><code>def try_find_processed_version(\n    dataset_id: str, language_id: str | None = None\n) -&gt; DatasetDict | Dataset | None:\n    \"\"\"\n    Try to load a processed version of the dataset if it exists locally. Check if:\n    1. The dataset_id is a local path to an already processed dataset directory.\n    or\n    2. The dataset_id is a path to a local dataset, but a processed version already exists locally.\n    or\n    3. The dataset_id is a HuggingFace dataset ID, but a processed version already exists locally.\n    \"\"\"\n    if Path(dataset_id).name == PROC_DATASET_DIR and Path(dataset_id).is_dir():\n        if (\n            Path(dataset_id + \"/train\").is_dir()\n            and Path(dataset_id + \"/test\").is_dir()\n            and Path(dataset_id + \"/dataset_dict.json\").is_file()\n        ):\n            return load_from_disk(dataset_id)\n        else:\n            raise FileNotFoundError(\"Processed dataset is incomplete.\")\n\n    proc_dataset_path = _get_local_proc_dataset_path(dataset_id)\n    if Path(proc_dataset_path).is_dir():\n        return load_from_disk(proc_dataset_path)\n\n    hf_proc_dataset_path = _get_hf_proc_dataset_path(dataset_id, language_id)\n    if Path(hf_proc_dataset_path).is_dir():\n        logger.info(\n            f\"Found processed dataset version at {hf_proc_dataset_path} of HF dataset {dataset_id}. \"\n            f\"Loading it directly and skipping processing again the original version.\"\n        )\n        return load_from_disk(hf_proc_dataset_path)\n\n    return None\n</code></pre>"},{"location":"api/#speech_to_text_finetune.finetune_whisper","title":"<code>speech_to_text_finetune.finetune_whisper</code>","text":""},{"location":"api/#speech_to_text_finetune.finetune_whisper.run_finetuning","title":"<code>run_finetuning(config_path='config.yaml')</code>","text":"<p>Complete pipeline for preprocessing the Common Voice dataset and then finetuning a Whisper model on it.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>yaml filepath that follows the format defined in config.py</p> <code>'config.yaml'</code> <p>Returns:</p> Type Description <code>Tuple[Dict, Dict]</code> <p>Tuple[Dict, Dict]: evaluation metrics from the baseline and the finetuned models</p> Source code in <code>src/speech_to_text_finetune/finetune_whisper.py</code> <pre><code>def run_finetuning(\n    config_path: str = \"config.yaml\",\n) -&gt; Tuple[Dict, Dict]:\n    \"\"\"\n    Complete pipeline for preprocessing the Common Voice dataset and then finetuning a Whisper model on it.\n\n    Args:\n        config_path (str): yaml filepath that follows the format defined in config.py\n\n    Returns:\n        Tuple[Dict, Dict]: evaluation metrics from the baseline and the finetuned models\n    \"\"\"\n    cfg = load_config(config_path)\n\n    language_id = TO_LANGUAGE_CODE.get(cfg.language.lower())\n    if not language_id:\n        raise ValueError(\n            f\"\\nThis language is not inherently supported by this Whisper model. However you can still \u201cteach\u201d Whisper \"\n            f\"the language of your choice!\\nVisit https://glottolog.org/, find which language is most closely \"\n            f\"related to {cfg.language} from the list of supported languages below, and update your config file with \"\n            f\"that language.\\n{json.dumps(TO_LANGUAGE_CODE, indent=4, sort_keys=True)}.\"\n        )\n\n    if cfg.repo_name == \"default\":\n        cfg.repo_name = f\"{cfg.model_id.split('/')[1]}-{language_id}\"\n    local_output_dir = f\"./artifacts/{cfg.repo_name}\"\n\n    logger.info(f\"Finetuning starts soon, results saved locally at {local_output_dir}\")\n    hf_repo_name = \"\"\n    if cfg.training_hp.push_to_hub:\n        hf_username = get_hf_username()\n        hf_repo_name = f\"{hf_username}/{cfg.repo_name}\"\n        logger.info(\n            f\"Results will also be uploaded in HF at {hf_repo_name}. \"\n            f\"Private repo is set to {cfg.training_hp.hub_private_repo}.\"\n        )\n\n    device = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n    logger.info(\n        f\"Loading {cfg.model_id} on {device} and configuring it for {cfg.language}.\"\n    )\n    processor = WhisperProcessor.from_pretrained(\n        cfg.model_id, language=cfg.language, task=\"transcribe\"\n    )\n    model = WhisperForConditionalGeneration.from_pretrained(cfg.model_id)\n\n    # disable cache during training since it's incompatible with gradient checkpointing\n    model.config.use_cache = False\n    # set language and task for generation during inference and re-enable cache\n    model.generate = partial(\n        model.generate, language=cfg.language.lower(), task=\"transcribe\", use_cache=True\n    )\n\n    data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n\n    training_args = Seq2SeqTrainingArguments(\n        output_dir=local_output_dir,\n        hub_model_id=hf_repo_name,\n        report_to=[\"tensorboard\"],\n        **cfg.training_hp.model_dump(),\n    )\n\n    if proc_dataset := try_find_processed_version(\n        dataset_id=cfg.dataset_id, language_id=language_id\n    ):\n        logger.info(\n            f\"Loading processed dataset version of {cfg.dataset_id} and skipping processing.\"\n        )\n        dataset = proc_dataset\n        dataset[\"train\"] = load_subset_of_dataset(dataset[\"train\"], cfg.n_train_samples)\n        dataset[\"test\"] = load_subset_of_dataset(dataset[\"test\"], cfg.n_test_samples)\n    else:\n        logger.info(f\"Loading {cfg.dataset_id}. Language selected {cfg.language}\")\n        dataset, save_proc_dataset_dir = load_dataset_from_dataset_id(\n            dataset_id=cfg.dataset_id,\n            language_id=language_id,\n        )\n        dataset[\"train\"] = load_subset_of_dataset(dataset[\"train\"], cfg.n_train_samples)\n        dataset[\"test\"] = load_subset_of_dataset(dataset[\"test\"], cfg.n_test_samples)\n        logger.info(\"Processing dataset...\")\n        dataset = process_dataset(\n            dataset=dataset,\n            processor=processor,\n            batch_size=cfg.training_hp.per_device_train_batch_size,\n            proc_dataset_path=save_proc_dataset_dir,\n        )\n        logger.info(\n            f\"Processed dataset saved at {save_proc_dataset_dir}. Future runs of {cfg.dataset_id} will \"\n            f\"automatically use this processed version.\"\n        )\n\n    wer = evaluate.load(\"wer\")\n    cer = evaluate.load(\"cer\")\n\n    trainer = Seq2SeqTrainer(\n        args=training_args,\n        model=model,\n        train_dataset=dataset[\"train\"],\n        eval_dataset=dataset[\"test\"],\n        data_collator=data_collator,\n        compute_metrics=partial(\n            compute_wer_cer_metrics,\n            processor=processor,\n            wer=wer,\n            cer=cer,\n            normalizer=BasicTextNormalizer(),\n        ),\n        processing_class=processor.feature_extractor,\n    )\n\n    processor.save_pretrained(training_args.output_dir)\n\n    logger.info(\n        f\"Before finetuning, run evaluation on the baseline model {cfg.model_id} to easily compare performance\"\n        f\" before and after finetuning\"\n    )\n    baseline_eval_results = trainer.evaluate()\n    logger.info(f\"Baseline evaluation complete. Results:\\n\\t {baseline_eval_results}\")\n\n    logger.info(\n        f\"Start finetuning job on {dataset['train'].num_rows} audio samples. Monitor training metrics in real time in \"\n        f\"a local tensorboard server by running in a new terminal: tensorboard --logdir {training_args.output_dir}/runs\"\n    )\n    try:\n        trainer.train()\n    except KeyboardInterrupt:\n        logger.info(\"Stopping the finetuning job prematurely...\")\n    else:\n        logger.info(\"Finetuning job complete.\")\n\n    logger.info(f\"Start evaluation on {dataset['test'].num_rows} audio samples.\")\n    eval_results = trainer.evaluate()\n    logger.info(f\"Evaluation complete. Results:\\n\\t {eval_results}\")\n\n    if cfg.training_hp.push_to_hub:\n        logger.info(f\"Uploading model and eval results to HuggingFace: {hf_repo_name}\")\n        trainer.push_to_hub()\n        upload_custom_hf_model_card(\n            hf_repo_name=hf_repo_name,\n            model_id=cfg.model_id,\n            dataset_id=cfg.dataset_id,\n            language_id=language_id,\n            language=cfg.language,\n            n_train_samples=dataset[\"train\"].num_rows,\n            n_eval_samples=dataset[\"test\"].num_rows,\n            baseline_eval_results=baseline_eval_results,\n            ft_eval_results=eval_results,\n        )\n\n    logger.info(f\"Find your final, best performing model at {local_output_dir}\")\n    return baseline_eval_results, eval_results\n</code></pre>"},{"location":"api/#speech_to_text_finetune.utils","title":"<code>speech_to_text_finetune.utils</code>","text":""},{"location":"api/#speech_to_text_finetune.utils.compute_wer_cer_metrics","title":"<code>compute_wer_cer_metrics(pred, processor, wer, cer, normalizer)</code>","text":"<p>Word Error Rate (wer) is a metric that measures the ratio of errors the ASR model makes given a transcript to the total words spoken. Lower is better. Character Error Rate (cer) is similar to wer, but operates on character instead of word. This metric is better suited for languages with no concept of \"word\" like Chinese or Japanese. Lower is better.</p> <p>More info: https://huggingface.co/learn/audio-course/en/chapter5/fine-tuning#evaluation-metrics</p> <p>Note 1: WER/CER can be larger than 1.0, if the number of insertions I is larger than the number of correct words C. Note 2: WER/CER doesn't tell the whole story and is not fully representative of the quality of the ASR model.</p> <p>Parameters:</p> Name Type Description Default <code>pred</code> <code>EvalPrediction</code> <p>Transformers object that holds predicted tokens and ground truth labels</p> required <code>processor</code> <code>WhisperProcessor</code> <p>Whisper processor used to decode tokens to strings</p> required <code>wer</code> <code>EvaluationModule</code> <p>module that calls the computing function for WER</p> required <code>cer</code> <code>EvaluationModule</code> <p>module that calls the computing function for CER</p> required <code>normalizer</code> <code>BasicTextNormalizer</code> <p>Normalizer from Whisper</p> required <p>Returns:     wer (Dict): computed WER metric</p> Source code in <code>src/speech_to_text_finetune/utils.py</code> <pre><code>def compute_wer_cer_metrics(\n    pred: EvalPrediction,\n    processor: WhisperProcessor,\n    wer: EvaluationModule,\n    cer: EvaluationModule,\n    normalizer: BasicTextNormalizer,\n) -&gt; Dict:\n    \"\"\"\n    Word Error Rate (wer) is a metric that measures the ratio of errors the ASR model makes given a transcript to the\n    total words spoken. Lower is better.\n    Character Error Rate (cer) is similar to wer, but operates on character instead of word. This metric is better\n    suited for languages with no concept of \"word\" like Chinese or Japanese. Lower is better.\n\n    More info: https://huggingface.co/learn/audio-course/en/chapter5/fine-tuning#evaluation-metrics\n\n    Note 1: WER/CER can be larger than 1.0, if the number of insertions I is larger than the number of correct words C.\n    Note 2: WER/CER doesn't tell the whole story and is not fully representative of the quality of the ASR model.\n\n    Args:\n        pred (EvalPrediction): Transformers object that holds predicted tokens and ground truth labels\n        processor (WhisperProcessor): Whisper processor used to decode tokens to strings\n        wer (EvaluationModule): module that calls the computing function for WER\n        cer (EvaluationModule): module that calls the computing function for CER\n        normalizer (BasicTextNormalizer): Normalizer from Whisper\n    Returns:\n        wer (Dict): computed WER metric\n    \"\"\"\n\n    pred_ids = pred.predictions\n    label_ids = pred.label_ids\n\n    # replace -100 with the pad_token_id\n    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n\n    # we do not want to group tokens when computing the metrics\n    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n\n    # compute orthographic wer\n    wer_ortho = 100 * wer.compute(predictions=pred_str, references=label_str)\n    cer_ortho = 100 * cer.compute(predictions=pred_str, references=label_str)\n\n    # compute normalised WER\n    pred_str_norm = [normalizer(pred) for pred in pred_str]\n    label_str_norm = [normalizer(label) for label in label_str]\n    # filtering step to only evaluate the samples that correspond to non-zero references:\n    pred_str_norm = [\n        pred_str_norm[i]\n        for i in range(len(pred_str_norm))\n        if len(label_str_norm[i]) &gt; 0\n    ]\n    label_str_norm = [\n        label_str_norm[i]\n        for i in range(len(label_str_norm))\n        if len(label_str_norm[i]) &gt; 0\n    ]\n\n    wer = 100 * wer.compute(predictions=pred_str_norm, references=label_str_norm)\n    cer = 100 * cer.compute(predictions=pred_str_norm, references=label_str_norm)\n\n    return {\"wer_ortho\": wer_ortho, \"wer\": wer, \"cer_ortho\": cer_ortho, \"cer\": cer}\n</code></pre>"},{"location":"api/#speech_to_text_finetune.utils.upload_custom_hf_model_card","title":"<code>upload_custom_hf_model_card(hf_repo_name, model_id, dataset_id, language_id, language, n_train_samples, n_eval_samples, baseline_eval_results, ft_eval_results)</code>","text":"<p>Create and upload a custom Model Card (https://huggingface.co/docs/hub/model-cards) to the Hugging Face repo of the finetuned model that highlights the evaluation results before and after finetuning.</p> Source code in <code>src/speech_to_text_finetune/utils.py</code> <pre><code>def upload_custom_hf_model_card(\n    hf_repo_name: str,\n    model_id: str,\n    dataset_id: str,\n    language_id: str,\n    language: str,\n    n_train_samples: int,\n    n_eval_samples: int,\n    baseline_eval_results: Dict,\n    ft_eval_results: Dict,\n) -&gt; None:\n    \"\"\"\n    Create and upload a custom Model Card (https://huggingface.co/docs/hub/model-cards) to the Hugging Face repo\n    of the finetuned model that highlights the evaluation results before and after finetuning.\n    \"\"\"\n    card_metadata = ModelCardData(\n        model_name=f\"Finetuned {model_id} on {language}\",\n        base_model=model_id,\n        datasets=[dataset_id.split(\"/\")[-1]],\n        language=language_id,\n        license=\"apache-2.0\",\n        library_name=\"transformers\",\n        eval_results=[\n            EvalResult(\n                task_type=\"automatic-speech-recognition\",\n                task_name=\"Speech-to-Text\",\n                dataset_type=\"common_voice\",\n                dataset_name=f\"Common Voice ({language})\",\n                metric_type=\"wer\",\n                metric_value=round(ft_eval_results[\"eval_wer\"], 3),\n            )\n        ],\n    )\n    content = f\"\"\"\n---\n{card_metadata.to_yaml()}\n---\n\n# Finetuned {model_id} on {n_train_samples} {language} training audio samples from {dataset_id}.\n\nThis model was created from the Mozilla.ai Blueprint:\n[speech-to-text-finetune](https://github.com/mozilla-ai/speech-to-text-finetune).\n\n## Evaluation results on {n_eval_samples} audio samples of {language}:\n\n### Baseline model (before finetuning) on {language}\n- Word Error Rate (Normalized): {round(baseline_eval_results[\"eval_wer\"], 3)}\n- Word Error Rate (Orthographic): {round(baseline_eval_results[\"eval_wer_ortho\"], 3)}\n- Character Error Rate (Normalized): {round(baseline_eval_results[\"eval_cer\"], 3)}\n- Character Error Rate (Orthographic): {round(baseline_eval_results[\"eval_cer_ortho\"], 3)}\n- Loss: {round(baseline_eval_results[\"eval_loss\"], 3)}\n\n### Finetuned model (after finetuning) on {language}\n- Word Error Rate (Normalized): {round(ft_eval_results[\"eval_wer\"], 3)}\n- Word Error Rate (Orthographic): {round(ft_eval_results[\"eval_wer_ortho\"], 3)}\n- Character Error Rate (Normalized): {round(ft_eval_results[\"eval_cer\"], 3)}\n- Character Error Rate (Orthographic): {round(ft_eval_results[\"eval_cer_ortho\"], 3)}\n- Loss: {round(ft_eval_results[\"eval_loss\"], 3)}\n\"\"\"\n\n    card = ModelCard(content)\n    card.push_to_hub(hf_repo_name)\n</code></pre>"},{"location":"api/#speech_to_text_finetune.make_custom_dataset_app","title":"<code>speech_to_text_finetune.make_custom_dataset_app</code>","text":""},{"location":"api/#speech_to_text_finetune.make_custom_dataset_app.save_text_audio_to_file","title":"<code>save_text_audio_to_file(audio_input, sentence, dataset_dir, is_train_sample)</code>","text":"<p>Save the audio recording in a .wav file using the index of the text sentence in the filename. And save the associated text sentence in a .csv file using the same index.</p> <p>Parameters:</p> Name Type Description Default <code>audio_input</code> <code>Audio</code> <p>Gradio audio object to be converted to audio data and then saved to a .wav file</p> required <code>sentence</code> <code>str</code> <p>The text sentence that will be associated with the audio</p> required <code>dataset_dir</code> <code>str</code> <p>The dataset directory path to store the indexed sentences and the associated audio files</p> required <code>is_train_sample</code> <code>bool</code> <p>Whether to save the text-recording pair to the train or test dataset</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Status text for Gradio app</p> <code>None</code> <code>None</code> <p>Returning None here will reset the audio module to record again from scratch</p> Source code in <code>src/speech_to_text_finetune/make_custom_dataset_app.py</code> <pre><code>def save_text_audio_to_file(\n    audio_input: gr.Audio,\n    sentence: str,\n    dataset_dir: str,\n    is_train_sample: bool,\n) -&gt; Tuple[str, None]:\n    \"\"\"\n    Save the audio recording in a .wav file using the index of the text sentence in the filename.\n    And save the associated text sentence in a .csv file using the same index.\n\n    Args:\n        audio_input (gr.Audio): Gradio audio object to be converted to audio data and then saved to a .wav file\n        sentence (str): The text sentence that will be associated with the audio\n        dataset_dir (str): The dataset directory path to store the indexed sentences and the associated audio files\n        is_train_sample (bool): Whether to save the text-recording pair to the train or test dataset\n\n    Returns:\n        str: Status text for Gradio app\n        None: Returning None here will reset the audio module to record again from scratch\n    \"\"\"\n    Path(f\"{dataset_dir}/train\").mkdir(parents=True, exist_ok=True)\n    Path(f\"{dataset_dir}/train/clips\").mkdir(parents=True, exist_ok=True)\n    Path(f\"{dataset_dir}/test\").mkdir(parents=True, exist_ok=True)\n    Path(f\"{dataset_dir}/test/clips\").mkdir(parents=True, exist_ok=True)\n\n    data_path = (\n        Path(f\"{dataset_dir}/train/\")\n        if is_train_sample\n        else Path(f\"{dataset_dir}/test/\")\n    )\n    text_path = Path(f\"{data_path}/text.csv\")\n    if text_path.is_file():\n        df = pd.read_csv(text_path)\n    else:\n        df = pd.DataFrame(columns=[\"index\", \"sentence\"])\n\n    index = len(df)\n    text_df = pd.concat(\n        [df, pd.DataFrame([{\"index\": index, \"sentence\": sentence}])],\n        ignore_index=True,\n    )\n    text_df = text_df.sort_values(by=\"index\")\n    text_df.to_csv(text_path, index=False)\n\n    audio_filepath = f\"{data_path}/clips/rec_{index}.wav\"\n\n    sr, data = audio_input\n    sf.write(file=audio_filepath, data=data, samplerate=sr)\n\n    return (\n        f\"\"\"\u2705 Updated {text_path} \\n\u2705 Saved recording to {audio_filepath}\"\"\",\n        None,\n    )\n</code></pre>"},{"location":"customization/","title":"\ud83c\udfa8 Customization Guide","text":"<p>This Blueprint is designed to be flexible and easily adaptable to your specific needs. This guide will walk you through some key areas you can customize to make the Blueprint your own.</p>"},{"location":"customization/#byod-bring-your-own-dataset","title":"BYOD: Bring Your Own Dataset","text":"<p>But I already have my own speech-text dateset! I don't want to create a new one from scratch or use Common Voice! Does this Blueprint have anything to offer me?</p> <p>But of course!</p> <p>This guide will walk you through how to use the existing codebase to adapt it to your own unique dataset with minimal effort.</p> <p>The idea is to load and pre-process your own dataset in the same format as the existing datasets, allowing you to seamlessly integrate with the <code>finetune_whisper.py</code> script.</p>"},{"location":"customization/#step-1-understand-your-dataset","title":"Step 1: Understand your Dataset","text":"<p>Before creating your custom dataset loading function, it's essential to understand the data format that the <code>finetune_whisper.py</code> script expects. Typically, the dataset should have a structure that looks a bit like this:</p> <pre><code>{\n    \"train\": [\n        {\n            \"audio\": \"path/to/audio_file.wav\",\n            \"text\": \"The transcribed text of the audio\"\n        },\n        # More examples...\n    ],\n    \"test\": [\n        {\n            \"audio\": \"path/to/audio_file_2.wav\",\n            \"text\": \"Another transcribed text\"\n        },\n        # More examples...\n    ]\n}\n</code></pre> <p>Notably, there should be a pair of transcribed text and an audio clip (usually in the form of a path to the audio file, either <code>.mp3</code> or <code>.wav</code>)</p>"},{"location":"customization/#step-2-create-a-load_my_dataset-function","title":"Step 2: Create a load_my_dataset Function","text":"<p>Next, you'll create a custom dataset loading function and place it inside <code>data_process.py</code>. This function will load and pre-process your dataset into the expected format and return an HuggingFace <code>DatasetDict</code> containing two <code>Dataset</code> objects like so: <code>train</code>:<code>Dataset</code> and <code>test</code>:<code>Dataset</code> for each split respectively.</p> <p>As an example, lets consider that you have a directory with a csv file and all the audio clips like this:</p> <p><pre><code>datasets/\n\u251c\u2500\u2500 my_dataset/\n\u2502   \u251c\u2500\u2500 dataset.csv\n\u2502   \u2514\u2500\u2500 audio_files/\n\u2502       \u251c\u2500\u2500 audio_1.wav\n\u2502       \u251c\u2500\u2500 audio_2.wav\n\u2502       \u251c\u2500\u2500 audio_3.wav\n\u2502       \u2514\u2500\u2500 ...\n</code></pre> and that the .csv file has the following format:</p> <p>```csv my_dataset/dataset.csv audio,text example_1.mp3,\"This is an example\" example_2.mp3,\"This is another example\" ... example_n.mp3,\"This is yet another example\" <pre><code>**Example function**\n</code></pre> import os import pandas as pd from datasets import Dataset, DatasetDict from sklearn.model_selection import train_test_split</p> <p>def load_my_dataset(my_dataset_dir: str = \"/home/user/datasets/my_dataset\", train_split: float = 0.8) -&gt; DatasetDict:     \"\"\"     Load and process your custom dataset from the given directory.</p> <pre><code>Args:\n    my_dataset_dir (str): Path to the directory containing your dataset.\n    train_split (float): Percentage of data to use for training. The rest will be used for evaluation as a test set.\n\nReturns:\n    DatasetDict: HF Dataset dictionary that consists of two distinct Datasets (train and test)\n\"\"\"\n# Define the path to the CSV file\ncsv_path = os.path.join(my_dataset_dir, \"dataset.csv\")\ndf = pd.read_csv(csv_path)\n\n# Only keep the columns we need and drop any other possible metadata columns\ndf = df.select_columns([\"audio\", \"sentence\"])\n\n# Our processing script expects the column with the transcribed text to be called \"sentence\"\ndf = df.rename(columns={\"text\": \"sentence\"})\n\n# Replace the relative path to the audio clip with the absolute path\ndf[\"audio\"] = df[\"audio\"].apply(lambda p: f\"{my_dataset_dir}/audio_files/{p}\")\n\n# Split the DataFrame into train and test sets and set a seed to shuffle and easily reproduce\ntrain_df, test_df = train_test_split(df, train_size=train_split, random_state=42)\n\n# Return the DatasetDict containing the train and test datasets\nreturn DatasetDict({\n    \"train\": Dataset.from_pandas(train_df),\n    \"test\": Dataset.from_pandas(test_df)\n})\n</code></pre> <p><pre><code>### Step 3: Integrate with the rest of the codebase\n\nOnce you've created your custom dataset loading function, you need to integrate it into the existing codebase. Specifically, you need to update the `load_dataset_from_dataset_id` function in [data_process.py](../src/speech_to_text_finetune/data_process.py) to include the new function. Simply add to the function:\n</code></pre>     try:         dataset = load_my_dataset(dataset_id)         return dataset, _get_local_proc_dataset_path(dataset_id)     except FileNotFoundError:         pass <pre><code>### Step 4: Update your config file\n\nDon't forget to update your config file so that the `dataset_id` points to the right directory path:\n</code></pre> model_id: openai/whisper-tiny dataset_id: /home/user/datasets/my_dataset ... <pre><code>### Step 5: Fine-Tune the model with your own dataset!\n\nFinally, simply run the finetune_whisper.py script to fine-tune the Whisper model using your custom dataset.\n</code></pre> python src/speech_to_text_finetune/finetune_whisper.py ```</p>"},{"location":"customization/#contributing-to-the-blueprint","title":"\ud83e\udd1d Contributing to the Blueprint","text":"<p>Want to help improve or extend this Blueprint? Check out the Future Features &amp; Contributions Guide to see how you can contribute your ideas, code, or feedback to make this Blueprint even better!</p>"},{"location":"future-features-contributions/","title":"\ud83d\ude80 Future Features &amp; Contributions","text":"<p>This Blueprint is an evolving project designed to grow with the help of the open-source community. Whether you\u2019re an experienced developer or just starting, there are many ways you can contribute and help shape the future of this tool.</p>"},{"location":"future-features-contributions/#how-you-can-contribute","title":"\ud83c\udf1f How You Can Contribute","text":""},{"location":"future-features-contributions/#enhance-the-blueprint","title":"\ud83d\udee0\ufe0f Enhance the Blueprint","text":"<ul> <li>Check the Issues page to see if there are feature requests you'd like to implement</li> <li>Refer to our Contribution Guide for more details on contributions</li> </ul>"},{"location":"future-features-contributions/#extensibility-ideas","title":"\ud83c\udfa8 Extensibility Ideas","text":"<p>This Blueprint is designed to be a foundation you can build upon. By extending its capabilities, you can open the door to new applications, improve user experience, and adapt the Blueprint to address other use cases. Here are a few ideas for how you can expand its potential: - Add support for more types of models to finetune, such as w2v2-bert. More info here - Improve the training efficiency and speed using PEFT + LORA. More info here</p> <p>We\u2019d love to see how you can enhance this Blueprint! If you create improvements or extend its capabilities, consider contributing them back to the project so others in the community can benefit from your work. Check out our Contributions Guide to get started!</p>"},{"location":"future-features-contributions/#share-your-ideas","title":"\ud83d\udca1 Share Your Ideas","text":"<p>Got an idea for how this Blueprint could be improved? You can share your suggestions through GitHub Discussions.</p>"},{"location":"future-features-contributions/#build-new-blueprints","title":"\ud83c\udf0d Build New Blueprints","text":"<p>This project is part of a larger initiative to create a collection of reusable starter code solutions that use open-source AI tools. If you\u2019re inspired to create your own Blueprint, you can use the Blueprint-template to get started.</p> <p>Your contributions help make this Blueprint better for everyone \ud83c\udf89</p>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#get-started-with-speech-to-text-finetune-blueprint-using-one-of-the-options-below","title":"Get started with Speech-to-text-finetune Blueprint using one of the options below:","text":""},{"location":"getting-started/#setup-options","title":"Setup options","text":"\u2601\ufe0f Google Colab (GPU)\u2601\ufe0f GitHub Codespaces\ud83d\udcbb Local Installation <p>Finetune a STT model using CommonVoice data by launching the Google Colab notebook below</p> <p>Click the button below to launch the project directly in Google Colab:</p> <p><p></p></p> <p>Click the button below to launch the project directly in GitHub Codespaces:</p> <p><p></p></p> <p>Once the Codespaces environment launches, inside the terminal, install dependencies:</p> <pre><code>pip install -e .\n</code></pre> <p>To load the app for making your own dataset for STT finetuning:</p> <pre><code>python src/speech_to_text_finetune/make_custom_dataset_app.py\n</code></pre> <p>To load the Transcription app:</p> <pre><code>python demo/transcribe_app.py\n</code></pre> <p>To run the Finetuning job:</p> <pre><code>python src/speech_to_text_finetune/finetune_whisper.py\n</code></pre> <p>To install the project locally:</p> <pre><code>git clone https://github.com/mozilla-ai/speech-to-text-finetune.git\ncd speech-to-text-finetune\n</code></pre> <p>install dependencies:</p> <pre><code>pip install -e .\n</code></pre> <p>To load the app for making your own dataset for STT finetuning:</p> <pre><code>python src/speech_to_text_finetune/make_custom_dataset_app.py\n</code></pre> <p>To load the Transcription app:</p> <pre><code>python demo/transcribe_app.py\n</code></pre> <p>To run the Finetuning job:</p> <pre><code>python src/speech_to_text_finetune/finetune_whisper.py\n</code></pre>"},{"location":"getting-started/#troubleshooting","title":"Troubleshooting","text":"<p>Troubleshooting - TBA</p>"},{"location":"step-by-step-guide/","title":"Step-by-Step Guide: How the Speech-to-Text-Finetune Blueprint Works","text":"<p>This Blueprint enables you to fine-tune a Speech-to-Text (STT) model, using either your own data or the Common Voice dataset. This Step-by-Step guide you through the end-to-end process of finetuning an STT model based on your needs.</p>"},{"location":"step-by-step-guide/#overview","title":"Overview","text":"<p>This blueprint consists of three independent, yet complementary, components:</p> <ol> <li> <p>Transcription app \ud83c\udf99\ufe0f\ud83d\udcdd: A simple UI that lets you record your voice, pick any HF STT/ASR model, and get an instant transcription.</p> </li> <li> <p>Dataset maker app \ud83d\udcc2\ud83c\udfa4: Another UI app that enables you to easily and quickly create your own Speech-to-Text dataset.</p> </li> <li> <p>Finetuning script \ud83d\udee0\ufe0f\ud83e\udd16: A script to finetune your own STT model, either using Common Voice data or your own custom data created by the Dataset maker app.</p> </li> </ol>"},{"location":"step-by-step-guide/#step-by-step-guide","title":"Step-by-Step Guide","text":"<p>Visit the Getting Started page for the initial project setup.</p> <p>The following guide is a suggested user-flow for getting the most out of this Blueprint</p>"},{"location":"step-by-step-guide/#step-1-initial-transcription-testing","title":"Step 1 - Initial transcription testing","text":"<p>Start by initially testing the quality of the Speech-to-Text models available in HuggingFace:</p> <ol> <li> <p>Simply execute:</p> <pre><code>python demo/transcribe_app.py\n</code></pre> </li> <li> <p>Select or add the HF model id of your choice</p> </li> <li>Record a sample of your voice and get the transcribed text back. You may find that there are sometimes inaccuracies for your voice/accent/chosen language, indicating the model could benefit form finetuning on additional data.</li> </ol>"},{"location":"step-by-step-guide/#step-2-make-your-custom-dataset-for-stt-finetuning","title":"Step 2 - Make your Custom Dataset for STT finetuning","text":"<ol> <li> <p>Create your own, custom dataset by running this command and following the instructions:</p> <pre><code>python src/speech_to_text_finetune/make_custom_dataset_app.py\n</code></pre> </li> <li> <p>Follow the instruction in the app to create at least 10 audio samples, which will be saved locally.</p> </li> </ol>"},{"location":"step-by-step-guide/#step-3-creating-a-finetuned-stt-model-using-your-custom-data","title":"Step 3 - Creating a finetuned STT model using your custom data","text":"<ol> <li> <p>Configure <code>config.yaml</code> with the model, custom data directory and hyperparameters of your choice. Note that if you select <code>push_to_hub: True</code> you need to have an HF account and log in locally. For example:</p> <pre><code>model_id: openai/whisper-tiny\ndataset_id: example_data/custom\nlanguage: English\nrepo_name: default\n\ntraining_hp:\n    push_to_hub: False\n    hub_private_repo: True\n    ...\n</code></pre> </li> <li> <p>Finetune a model by running: <pre><code>python src/speech_to_text_finetune/finetune_whisper.py\n</code></pre></p> </li> </ol> <p>[!TIP] You can prematurely and gracefully stop the finetuning job by pressing CTRL+C. The rest of the code (evaluation, uploading the model) will execute as normal.</p>"},{"location":"step-by-step-guide/#step-4-optional-creating-a-finetuned-stt-model-using-commonvoice-data","title":"Step 4 - (Optional) Creating a finetuned STT model using CommonVoice data","text":"<p>1. 2. Go to https://commonvoice.mozilla.org/en/datasets, pick your language and dataset version and download the dataset 2. Move the zipped file under a directory of your choice and extract it 3. Configure <code>config.yaml</code> with the model, Common Voice dataset path and hyperparameters of your choice. For example:</p> <pre><code>model_id: openai/whisper-tiny\ndataset_id: path/to/common_voice_data/language_id\ndataset_source: custom\nlanguage: English\nrepo_name: default\n\ntraining_hp:\n    push_to_hub: False\n    hub_private_repo: True\n    ...\n</code></pre> <ol> <li>Finetune a model by running: <pre><code>python src/speech_to_text_finetune/finetune_whisper.py\n</code></pre></li> </ol> <p>[!NOTE] Every time you load a new dataset, the script will have to process it before feeding it to the STT model. The script will then also save this processed dataset version locally so that next time you want to finetune a model on the same dataset, the processing step will be skipped, saving time &amp; computation.</p>"},{"location":"step-by-step-guide/#step-5-evaluate-transcription-accuracy-with-your-finetuned-stt-model","title":"Step 5 - Evaluate transcription accuracy with your finetuned STT model","text":"<ol> <li>Start the Transcription app:  ```bash python demo/transcribe_app.py <pre><code>2. Provided that `push_to_hub: True` when you Finetuned, you can select your HuggingFace model-id. If not, you can specify the local path to your model\n3. Record a sample of your voice and get the transcribed text back.\n4. You can easily switch between models with the same recorded sample to evaluate if the finetuned model has improved transcription accuracy.\n\n\n### Step 6 - Compare transcription performance between two models\n\n1. Start the Model Comparison app:\n ```bash\npython demo/model_comparison_app.py\n</code></pre></li> <li>Select a baseline model, for example the model you used as a base for finetuning.</li> <li>Select a comparison model, for example your finetuned model.</li> <li>Record a sample of your voice and get two transcriptions back side-by-side for an easier manual evaluation.</li> </ol>"},{"location":"step-by-step-guide/#step-7-evaluate-a-model-on-the-fleurs-dataset-on-a-specific-language","title":"Step 7 - Evaluate a model on the Fleurs dataset on a specific language","text":"<ol> <li>Configure the arguments through the command line according to your needs and execute the command below  <code>bash python evaluate_whisper.py --model_id openai/whisper-tiny --lang_code sw_ke --language Swahili --eval_batch_size 8 --n_test_samples -1 --fp16 True</code></li> </ol>"},{"location":"step-by-step-guide/#customizing-the-blueprint","title":"\ud83c\udfa8 Customizing the Blueprint","text":"<p>To better understand how you can tailor this Blueprint to suit your specific needs, please visit the Customization Guide.</p>"},{"location":"step-by-step-guide/#contributing-to-the-blueprint","title":"\ud83e\udd1d Contributing to the Blueprint","text":"<p>Want to help improve or extend this Blueprint? Check out the Future Features &amp; Contributions Guide to see how you can contribute your ideas, code, or feedback to make this Blueprint even better!</p>"},{"location":"step-by-step-guide/#resources-references","title":"\ud83d\udcd6 Resources &amp; References","text":"<p>If you are interested in learning more about this topic, you might find the following resources helpful: - Fine-Tune Whisper For Multilingual ASR with \ud83e\udd17 Transformers (Blog post by HuggingFace which inspired the implementation of the Blueprint!)</p> <ul> <li> <p>Automatic Speech Recognition Course from HuggingFace (Series of Blog posts)</p> </li> <li> <p>Fine-Tuning ASR Models: Key Definitions, Mechanics, and Use Cases (Blog post by Gladia)</p> </li> <li> <p>Active Learning Approach for Fine-Tuning Pre-Trained ASR Model for a low-resourced Language (Paper)</p> </li> <li> <p>Exploration of Whisper fine-tuning strategies for low-resource ASR (Paper)</p> </li> <li> <p>Finetuning Pretrained Model with Embedding of Domain and Language Information for ASR of Very Low-Resource Settings (Paper)</p> </li> </ul>"}]}