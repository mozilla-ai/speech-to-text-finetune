model_id: openai/whisper-tiny
dataset_id: example_data/custom
dataset_source: local
language: English
repo_name: default

training_hp:
    push_to_hub: False
    hub_private_repo: True
    num_train_epochs: 3
    per_device_train_batch_size: 64
    gradient_accumulation_steps: 1
    learning_rate: 1e-5
    warmup_steps: 50
    gradient_checkpointing: True
    fp16: False  # If a GPU is available, set it to True for faster training & decreased memory usage -> bigger batches
    eval_strategy: steps
    per_device_eval_batch_size: 8
    predict_with_generate: True
    generation_max_length: 225
    save_steps: 5
    logging_steps: 5
    load_best_model_at_end: True
    save_total_limit: 1
    metric_for_best_model: wer
    greater_is_better: False
